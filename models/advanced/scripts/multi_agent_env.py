"""
Multi-Agent —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Å–µ—Ç—å—é —Å–≤–µ—Ç–æ—Ñ–æ—Ä–æ–≤
–ö–∞–∂–¥—ã–π –ø–µ—Ä–µ–∫—Ä–µ—Å—Ç–æ–∫ —É–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã–º –∞–≥–µ–Ω—Ç–æ–º
"""
import os
import sys
import numpy as np
import traci
import gymnasium as gym
from gymnasium import spaces
from stable_baselines3 import DQN
from pathlib import Path

PROJECT_ROOT = Path(__file__).resolve().parents[3]

if 'SUMO_HOME' in os.environ:
    tools = os.path.join(os.environ['SUMO_HOME'], 'tools')
    sys.path.append(tools)
else:
    sys.exit("–£—Å—Ç–∞–Ω–æ–≤–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è SUMO_HOME")


class SingleIntersectionAgent:
    """
    –ê–≥–µ–Ω—Ç –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –æ–¥–Ω–∏–º –ø–µ—Ä–µ–∫—Ä–µ—Å—Ç–∫–æ–º
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç—É –∂–µ –ª–æ–≥–∏–∫—É —á—Ç–æ –∏ –≤ simple –º–æ–¥–µ–ª–∏
    """

    def __init__(self, tl_id, model=None):
        self.tl_id = tl_id
        self.model = model

        # –°–æ—Å—Ç–æ—è–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞
        self.current_phase = 0
        self.time_since_phase_start = 0
        self.in_yellow = False
        self.yellow_counter = 0
        self.yellow_duration = 3
        self.min_green_time = 10

        # Observation space: [vehicles_ns, vehicles_ew, bus_ns, bus_ew, phase]
        self.observation_space = spaces.Box(
            low=0, high=50, shape=(5,), dtype=np.float32
        )

        # Action space: 0 = –¥–µ—Ä–∂–∞—Ç—å, 1 = –ø–µ—Ä–µ–∫–ª—é—á–∏—Ç—å
        self.action_space = spaces.Discrete(2)

        # –î–ª—è –Ω–∞–≥—Ä–∞–¥—ã
        self.prev_total_waiting = 0

    def get_incoming_lanes(self):
        """–ü–æ–ª—É—á–∏—Ç—å –≤—Ö–æ–¥—è—â–∏–µ –ø–æ–ª–æ—Å—ã –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–∫—Ä–µ—Å—Ç–∫–∞"""
        try:
            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –≤—Ö–æ–¥—è—â–∏–µ –ø–æ–ª–æ—Å—ã –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —Å–≤–µ—Ç–æ—Ñ–æ—Ä–∞
            controlled_lanes = traci.trafficlight.getControlledLanes(self.tl_id)
            # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
            return list(set(controlled_lanes))
        except:
            return []

    def get_observation(self):
        """–ù–∞–±–ª—é–¥–µ–Ω–∏–µ –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–∫—Ä–µ—Å—Ç–∫–∞"""
        try:
            lanes = self.get_incoming_lanes()

            ns_vehicles = 0
            ew_vehicles = 0
            has_bus_ns = 0
            has_bus_ew = 0

            for lane in lanes:
                vehicles = traci.lane.getLastStepVehicleIDs(lane)

                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø–æ –∏–º–µ–Ω–∏ –ø–æ–ª–æ—Å—ã
                if 'v_' in lane:  # –í–µ—Ä—Ç–∏–∫–∞–ª—å–Ω–∞—è –¥–æ—Ä–æ–≥–∞ (—Å–µ–≤–µ—Ä-—é–≥)
                    ns_vehicles += len(vehicles)
                    for veh_id in vehicles:
                        if traci.vehicle.getTypeID(veh_id) == 'bus':
                            has_bus_ns = 1
                            break
                else:  # –ì–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω–∞—è –¥–æ—Ä–æ–≥–∞ (–≤–æ—Å—Ç–æ–∫-–∑–∞–ø–∞–¥)
                    ew_vehicles += len(vehicles)
                    for veh_id in vehicles:
                        if traci.vehicle.getTypeID(veh_id) == 'bus':
                            has_bus_ew = 1
                            break

            obs = np.array([
                min(ns_vehicles, 50),
                min(ew_vehicles, 50),
                has_bus_ns,
                has_bus_ew,
                self.current_phase
            ], dtype=np.float32)

            return obs
        except:
            return np.zeros(5, dtype=np.float32)

    def get_reward(self):
        """–ù–∞–≥—Ä–∞–¥–∞ –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–∫—Ä–µ—Å—Ç–∫–∞"""
        try:
            lanes = self.get_incoming_lanes()

            total_waiting = 0

            for lane in lanes:
                vehicles = traci.lane.getLastStepVehicleIDs(lane)
                for veh_id in vehicles:
                    waiting = traci.vehicle.getWaitingTime(veh_id)

                    if traci.vehicle.getTypeID(veh_id) == 'bus':
                        total_waiting += waiting * 3  # –ê–≤—Ç–æ–±—É—Å—ã –≤–∞–∂–Ω–µ–µ
                    else:
                        total_waiting += waiting

            # –ù–∞–≥—Ä–∞–¥–∞ = —É–ª—É—á—à–µ–Ω–∏–µ
            delta_waiting = self.prev_total_waiting - total_waiting
            reward = delta_waiting / 100.0

            # –ë–æ–Ω—É—Å—ã –∑–∞ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è
            if not self.in_yellow:
                ns_count = sum([len(traci.lane.getLastStepVehicleIDs(l))
                                for l in lanes if 'v_' in l])
                ew_count = sum([len(traci.lane.getLastStepVehicleIDs(l))
                                for l in lanes if 'h_' in l])

                if self.current_phase == 0 and ns_count > ew_count + 5:
                    reward += 1.0
                elif self.current_phase == 1 and ew_count > ns_count + 5:
                    reward += 1.0

                if self.current_phase == 0 and ew_count > ns_count + 10:
                    reward -= 2.0
                elif self.current_phase == 1 and ns_count > ew_count + 10:
                    reward -= 2.0

            self.prev_total_waiting = total_waiting

            return reward
        except:
            return 0.0

    def execute_action(self, action):
        """–í—ã–ø–æ–ª–Ω–∏—Ç—å –¥–µ–π—Å—Ç–≤–∏–µ –Ω–∞ –ø–µ—Ä–µ–∫—Ä–µ—Å—Ç–∫–µ"""
        if self.in_yellow:
            self.yellow_counter += 1
            if self.yellow_counter >= self.yellow_duration:
                self.current_phase = 1 - self.current_phase
                traci.trafficlight.setPhase(self.tl_id, self.current_phase * 2)
                self.in_yellow = False
                self.yellow_counter = 0
                self.time_since_phase_start = 0
        else:
            if action == 1 and self.time_since_phase_start >= self.min_green_time:
                traci.trafficlight.setPhase(self.tl_id, 1)  # –ñ–µ–ª—Ç—ã–π
                self.in_yellow = True
                self.yellow_counter = 0

            self.time_since_phase_start += 1

    def reset(self):
        """–°–±—Ä–æ—Å —Å–æ—Å—Ç–æ—è–Ω–∏—è –∞–≥–µ–Ω—Ç–∞"""
        self.current_phase = 0
        self.time_since_phase_start = 0
        self.in_yellow = False
        self.yellow_counter = 0
        self.prev_total_waiting = 0
        try:
            traci.trafficlight.setPhase(self.tl_id, 0)
        except:
            pass


class MultiAgentTrafficEnv(gym.Env):
    """
    Multi-Agent —Å—Ä–µ–¥–∞ –¥–ª—è —Å–µ—Ç–∏ –ø–µ—Ä–µ–∫—Ä–µ—Å—Ç–∫–æ–≤
    """

    def __init__(self, sumo_cfg, gui=False, route_file=None, use_pretrained=True):
        super().__init__()

        self.sumo_cfg = sumo_cfg
        self.route_file = route_file
        self.gui = gui
        self.sumo_cmd = None
        self.step_count = 0

        # –°–ø–∏—Å–æ–∫ ID –≤—Å–µ—Ö —Å–≤–µ—Ç–æ—Ñ–æ—Ä–æ–≤ –≤ —Å–µ—Ç–∏ (3x3 —Å–µ—Ç–∫–∞)
        self.traffic_lights = [
            'tl_00', 'tl_01', 'tl_02',
            'tl_10', 'tl_11', 'tl_12',
            'tl_20', 'tl_21', 'tl_22'
        ]

        # –°–æ–∑–¥–∞–µ–º –∞–≥–µ–Ω—Ç–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–µ—Ä–µ–∫—Ä–µ—Å—Ç–∫–∞
        self.agents = {}

        if use_pretrained:
            # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –∏–∑ simple
            try:
                simple_model = DQN.load(f"{PROJECT_ROOT}/models/simple/model/green_corridor_model")
                print("‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –∏–∑ simple/")
                for tl_id in self.traffic_lights:
                    self.agents[tl_id] = SingleIntersectionAgent(tl_id, model=simple_model)
            except:
                print("‚ö†Ô∏è –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤")
                for tl_id in self.traffic_lights:
                    self.agents[tl_id] = SingleIntersectionAgent(tl_id)
        else:
            # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤ –±–µ–∑ –º–æ–¥–µ–ª–∏
            for tl_id in self.traffic_lights:
                self.agents[tl_id] = SingleIntersectionAgent(tl_id)

        # –ü—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –¥–ª—è multi-agent (concatenated observations)
        single_obs_dim = 5
        self.observation_space = spaces.Box(
            low=0, high=50,
            shape=(len(self.traffic_lights) * single_obs_dim,),
            dtype=np.float32
        )

        # Action space: –ø–æ –æ–¥–Ω–æ–º—É –¥–µ–π—Å—Ç–≤–∏—é –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∞–≥–µ–Ω—Ç–∞
        self.action_space = spaces.MultiDiscrete([2] * len(self.traffic_lights))

    def reset(self, seed=None, options=None):
        super().reset(seed=seed)

        if self.sumo_cmd is not None:
            try:
                traci.close()
            except:
                pass

        sumo_binary = "sumo-gui" if self.gui else "sumo"
        self.sumo_cmd = [
            sumo_binary, "-c", self.sumo_cfg,
            "--start", "--quit-on-end",
            "--waiting-time-memory", "1000",
            "--time-to-teleport", "-1",
            "--no-warnings", "true"
        ]

        if self.route_file:
            self.sumo_cmd.extend(["--route-files", self.route_file])

        traci.start(self.sumo_cmd)

        # –°–±—Ä–æ—Å –≤—Å–µ—Ö –∞–≥–µ–Ω—Ç–æ–≤
        for agent in self.agents.values():
            agent.reset()

        self.step_count = 0

        return self._get_observation(), {}

    def _get_observation(self):
        """–°–æ–±–∏—Ä–∞–µ–º –Ω–∞–±–ª—é–¥–µ–Ω–∏—è –æ—Ç –≤—Å–µ—Ö –∞–≥–µ–Ω—Ç–æ–≤"""
        obs_list = []
        for tl_id in self.traffic_lights:
            agent_obs = self.agents[tl_id].get_observation()
            obs_list.extend(agent_obs)
        return np.array(obs_list, dtype=np.float32)

    def _get_reward(self):
        """–°—É–º–º–∞—Ä–Ω–∞—è –Ω–∞–≥—Ä–∞–¥–∞ –æ—Ç –≤—Å–µ—Ö –∞–≥–µ–Ω—Ç–æ–≤"""
        total_reward = 0
        for agent in self.agents.values():
            total_reward += agent.get_reward()
        return total_reward

    def step(self, actions):
        """
        –í—ã–ø–æ–ª–Ω–∏—Ç—å –¥–µ–π—Å—Ç–≤–∏—è –≤—Å–µ—Ö –∞–≥–µ–Ω—Ç–æ–≤
        actions: —Å–ø–∏—Å–æ–∫ –¥–µ–π—Å—Ç–≤–∏–π –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∞–≥–µ–Ω—Ç–∞
        """
        self.step_count += 1

        # –ö–∞–∂–¥—ã–π –∞–≥–µ–Ω—Ç –≤—ã–ø–æ–ª–Ω—è–µ—Ç —Å–≤–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ
        for i, tl_id in enumerate(self.traffic_lights):
            if isinstance(actions, np.ndarray):
                action = actions[i]
            else:
                action = actions
            self.agents[tl_id].execute_action(action)

        # –®–∞–≥ —Å–∏–º—É–ª—è—Ü–∏–∏
        traci.simulationStep()

        # –ü–æ–ª—É—á–∞–µ–º –Ω–∞–±–ª—é–¥–µ–Ω–∏—è –∏ –Ω–∞–≥—Ä–∞–¥—É
        obs = self._get_observation()
        reward = self._get_reward()

        terminated = traci.simulation.getMinExpectedNumber() <= 0
        truncated = self.step_count >= 1500  # –ë–æ–ª—å—à–µ –≤—Ä–µ–º–µ–Ω–∏ –¥–ª—è –±–æ–ª—å—à–æ–π —Å–µ—Ç–∏

        return obs, reward, terminated, truncated, {}

    def close(self):
        if self.sumo_cmd is not None:
            try:
                traci.close()
            except:
                pass


def train_multi_agent(total_steps=200000):
    """
    –û–±—É—á–µ–Ω–∏–µ multi-agent —Å–∏—Å—Ç–µ–º—ã
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–±—â—É—é –ø–æ–ª–∏—Ç–∏–∫—É –¥–ª—è –≤—Å–µ—Ö –∞–≥–µ–Ω—Ç–æ–≤
    """
    print("\n" + "=" * 70)
    print("üöÄ –û–ë–£–ß–ï–ù–ò–ï MULTI-AGENT –°–ò–°–¢–ï–ú–´")
    print("=" * 70)
    print("–°–µ—Ç—å: 3x3 –ø–µ—Ä–µ–∫—Ä–µ—Å—Ç–∫–∞ (9 —Å–≤–µ—Ç–æ—Ñ–æ—Ä–æ–≤)")
    print("–°—Ç—Ä–∞—Ç–µ–≥–∏—è: –ö–∞–∂–¥—ã–π –∞–≥–µ–Ω—Ç –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–±—â—É—é –ø–æ–ª–∏—Ç–∏–∫—É")
    print("=" * 70)

    env = MultiAgentTrafficEnv(
        f"{PROJECT_ROOT}/models/advanced/xmls/advanced.sumocfg",
        gui=False,
        route_file=f"{PROJECT_ROOT}/models/advanced/xmls/advanced.rou.xml",
        use_pretrained=True  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –∏–∑ simple
    )

    # –í–ê–ñ–ù–û: –î–ª—è multi-agent –∏—Å–ø–æ–ª—å–∑—É–µ–º –û–î–ù–£ –æ–±—â—É—é –º–æ–¥–µ–ª—å
    # –û–Ω–∞ –±—É–¥–µ—Ç –ø—Ä–∏–º–µ–Ω—è—Ç—å—Å—è –∫ –∫–∞–∂–¥–æ–º—É –ø–µ—Ä–µ–∫—Ä–µ—Å—Ç–∫—É –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ
    model = DQN(
        "MlpPolicy",
        env,
        learning_rate=0.0003,
        buffer_size=200000,
        learning_starts=10000,
        batch_size=64,
        tau=0.01,
        gamma=0.98,
        train_freq=4,
        target_update_interval=1000,
        exploration_fraction=0.3,
        exploration_initial_eps=1.0,
        exploration_final_eps=0.05,
        verbose=1
    )

    print(f"\nüéì –û–±—É—á–µ–Ω–∏–µ –Ω–∞ {total_steps} —à–∞–≥–æ–≤...")
    print("–ú–æ–¥–µ–ª—å –±—É–¥–µ—Ç —É—á–∏—Ç—å—Å—è —É–ø—Ä–∞–≤–ª—è—Ç—å –≤—Å–µ–º–∏ 9 –ø–µ—Ä–µ–∫—Ä–µ—Å—Ç–∫–∞–º–∏!")

    from stable_baselines3.common.callbacks import BaseCallback

    class MultiAgentCallback(BaseCallback):
        def __init__(self, verbose=0):
            super().__init__(verbose)
            self.episode_rewards = []
            self.current_reward = 0

        def _on_step(self):
            self.current_reward += self.locals['rewards'][0]

            if self.locals['dones'][0]:
                self.episode_rewards.append(self.current_reward)

                if len(self.episode_rewards) % 10 == 0:
                    recent = self.episode_rewards[-20:]
                    avg = np.mean(recent)
                    print(f"\nüìä Episode {len(self.episode_rewards)}: "
                          f"Avg Reward={avg:.1f}")

                self.current_reward = 0

            return True

    callback = MultiAgentCallback()

    model.learn(
        total_timesteps=total_steps,
        callback=callback
    )

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
    os.makedirs(f"{PROJECT_ROOT}/models/advanced/model", exist_ok=True)
    model.save(f"{PROJECT_ROOT}/models/advanced/model/multi_agent_model")
    print("\n‚úÖ Multi-agent –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞!")

    env.close()


def test_multi_agent():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ multi-agent —Å–∏—Å—Ç–µ–º—ã"""
    print("\n" + "=" * 70)
    print("üß™ –¢–ï–°–¢ MULTI-AGENT –°–ò–°–¢–ï–ú–´")
    print("=" * 70)

    try:
        model = DQN.load(f"{PROJECT_ROOT}/models/advanced/model/multi_agent_model")
    except:
        print("‚ö†Ô∏è Multi-agent –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        print("–ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –∏–∑ simple/")
        model = DQN.load(f"{PROJECT_ROOT}/models/simple/model/green_corridor_model")

    env = MultiAgentTrafficEnv(
        f"{PROJECT_ROOT}/models/advanced/xmls/advanced.sumocfg",
        gui=True,
        route_file=f"{PROJECT_ROOT}/models/advanced/xmls/advanced.rou.xml",
        use_pretrained=False
    )

    # –ü—Ä–∏—Å–≤–∞–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å –∫–∞–∂–¥–æ–º—É –∞–≥–µ–Ω—Ç—É
    for agent in env.agents.values():
        agent.model = model

    obs, _ = env.reset()
    total_reward = 0

    print("\nüëÄ –°–º–æ—Ç—Ä–∏ –≤ SUMO GUI:")
    print("   ‚Ä¢ 9 –ø–µ—Ä–µ–∫—Ä–µ—Å—Ç–∫–æ–≤ —Ä–∞–±–æ—Ç–∞—é—Ç –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ")
    print("   ‚Ä¢ –ö–∞–∂–¥—ã–π –∞–≥–µ–Ω—Ç –ø—Ä–∏–Ω–∏–º–∞–µ—Ç —Å–≤–æ–∏ —Ä–µ—à–µ–Ω–∏—è")
    print("   ‚Ä¢ –ó–µ–ª–µ–Ω—ã–µ = –∞–≤—Ç–æ–±—É—Å—ã, –ñ–µ–ª—Ç—ã–µ = –º–∞—à–∏–Ω—ã")

    for step in range(1000):
        # –ö–∞–∂–¥—ã–π –∞–≥–µ–Ω—Ç –ø—Ä–∏–Ω–∏–º–∞–µ—Ç —Ä–µ—à–µ–Ω–∏–µ
        actions = []
        for tl_id in env.traffic_lights:
            agent = env.agents[tl_id]
            agent_obs = agent.get_observation()
            action, _ = model.predict(agent_obs, deterministic=True)
            actions.append(action)

        obs, reward, terminated, truncated, _ = env.step(np.array(actions))
        total_reward += reward

        if terminated or truncated:
            break

    print(f"\n‚úÖ –¢–µ—Å—Ç –∑–∞–≤–µ—Ä—à–µ–Ω! –ù–∞–≥—Ä–∞–¥–∞: {total_reward:.2f}")
    env.close()


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument("--mode", type=str, default="train",
                        choices=["train", "test"])
    parser.add_argument("--steps", type=int, default=200000)
    args = parser.parse_args()

    if args.mode == "train":
        train_multi_agent(total_steps=args.steps)
    else:
        test_multi_agent()