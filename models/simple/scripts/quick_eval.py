import os
import sys
import numpy as np
from stable_baselines3 import DQN
from green_corridor import TrafficEnv
from models.simple.scripts import PROJECT_ROOT

if 'SUMO_HOME' in os.environ:
    tools = os.path.join(os.environ['SUMO_HOME'], 'tools')
    sys.path.append(tools)


def evaluate_agent(agent_type="trained", n_episodes=5):
    """
    –û—Ü–µ–Ω–∏–≤–∞–µ—Ç –∞–≥–µ–Ω—Ç–∞

    agent_type: "trained" (–æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å) –∏–ª–∏ "random" (—Å–ª—É—á–∞–π–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—è)
    """
    print(f"\n{'=' * 60}")
    print(f"üß™ –¢–µ—Å—Ç–∏—Ä—É–µ–º: {agent_type.upper()}")
    print('=' * 60)

    if agent_type == "trained":
        try:
            model = DQN.load(f"{PROJECT_ROOT}/models/simple/xmls/green_corridor_model")
        except FileNotFoundError:
            print("‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞! –°–Ω–∞—á–∞–ª–∞ –æ–±—É—á–∏ –º–æ–¥–µ–ª—å:")
            print("   python green_corridor.py --mode train --steps 50000")
            return None

    env = TrafficEnv("simple.sumocfg", gui=False, route_file="../xmls/simple.rou.xml")

    all_rewards = []
    all_bus_waiting = []
    all_car_waiting = []

    for episode in range(n_episodes):
        obs, _ = env.reset()
        episode_reward = 0
        done = False
        steps = 0

        while not done and steps < 300:
            if agent_type == "trained":
                action, _ = model.predict(obs, deterministic=True)
            else:  # random
                action = env.action_space.sample()

            obs, reward, terminated, truncated, _ = env.step(action)
            episode_reward += reward
            done = terminated or truncated
            steps += 1

        all_rewards.append(episode_reward)

        # –°–æ–±–∏—Ä–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É (–ø—Ä–∏–º–µ—Ä–Ω–∞—è, —Ç.–∫. —Å–∏–º—É–ª—è—Ü–∏—è —É–∂–µ –∑–∞–∫–æ–Ω—á–µ–Ω–∞)
        print(f"  Episode {episode + 1}/{n_episodes}: reward = {episode_reward:.1f}")

    env.close()

    avg_reward = np.mean(all_rewards)
    std_reward = np.std(all_rewards)

    print(f"\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã:")
    print(f"   –°—Ä–µ–¥–Ω—è—è –Ω–∞–≥—Ä–∞–¥–∞: {avg_reward:.1f} ¬± {std_reward:.1f}")
    print(f"   –õ—É—á—à–∏–π —ç–ø–∏–∑–æ–¥: {max(all_rewards):.1f}")
    print(f"   –•—É–¥—à–∏–π —ç–ø–∏–∑–æ–¥: {min(all_rewards):.1f}")

    return {
        'avg_reward': avg_reward,
        'std_reward': std_reward,
        'rewards': all_rewards
    }


def compare_agents():
    """–°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç –æ–±—É—á–µ–Ω–Ω–æ–≥–æ –∞–≥–µ–Ω—Ç–∞ —Å–æ —Å–ª—É—á–∞–π–Ω—ã–º"""
    print("\n" + "=" * 60)
    print("üéØ –ë–´–°–¢–†–ê–Ø –û–¶–ï–ù–ö–ê –ö–ê–ß–ï–°–¢–í–ê –ú–û–î–ï–õ–ò")
    print("=" * 60)
    print("–°—Ä–∞–≤–Ω–∏–≤–∞–µ–º –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å —Å–æ —Å–ª—É—á–∞–π–Ω—ã–º –∞–≥–µ–Ω—Ç–æ–º...")

    random_results = evaluate_agent("random", n_episodes=5)

    if random_results is None:
        return

    trained_results = evaluate_agent("trained", n_episodes=5)

    if trained_results is None:
        return

    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ
    improvement = ((trained_results['avg_reward'] - random_results['avg_reward'])
                   / abs(random_results['avg_reward']) * 100)

    print("\n" + "=" * 60)
    print("üìà –°–†–ê–í–ù–ï–ù–ò–ï")
    print("=" * 60)
    print(f"–°–ª—É—á–∞–π–Ω—ã–π –∞–≥–µ–Ω—Ç:  {random_results['avg_reward']:.1f}")
    print(f"–û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å: {trained_results['avg_reward']:.1f}")
    print(f"–£–ª—É—á—à–µ–Ω–∏–µ:        {improvement:+.1f}%")
    print("=" * 60)

    # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
    if improvement > 50:
        print("\n‚úÖ –û–¢–õ–ò–ß–ù–û! –ú–æ–¥–µ–ª—å —Ä–∞–±–æ—Ç–∞–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ª—É—á—à–µ!")
        print("   –ú–æ–∂–µ—à—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è –ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏–∏.")
    elif improvement > 20:
        print("\n‚úÖ –•–û–†–û–®–û! –ú–æ–¥–µ–ª—å –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —É–ª—É—á—à–µ–Ω–∏–µ.")
        print("   –ú–æ–∂–Ω–æ –æ–±—É—á–∏—Ç—å –¥–æ–ª—å—à–µ –¥–ª—è –ª—É—á—à–µ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞.")
    elif improvement > 0:
        print("\n‚ö†Ô∏è –°–õ–ê–ë–û. –ú–æ–¥–µ–ª—å —Ä–∞–±–æ—Ç–∞–µ—Ç —á—É—Ç—å –ª—É—á—à–µ —Å–ª—É—á–∞–π–Ω–æ–≥–æ.")
        print("   –ù—É–∂–Ω–æ –±–æ–ª—å—à–µ –æ–±—É—á–µ–Ω–∏—è: --steps 200000")
    else:
        print("\n‚ùå –ü–õ–û–•–û. –ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–∏–ª–∞—Å—å.")
        print("   –ó–∞–ø—É—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏–µ –∑–∞–Ω–æ–≤–æ —Å –±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —à–∞–≥–æ–≤.")

    print("=" * 60)

    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    if improvement < 30:
        print("\nüí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:")
        print("   1. –û–±—É—á–∏ –º–æ–¥–µ–ª—å –¥–æ–ª—å—à–µ:")
        print("      python green_corridor.py --mode train --steps 200000")
        print("   2. –ü—Ä–æ–≤–µ—Ä—å —á—Ç–æ —Ñ–∞–π–ª—ã –º–∞—Ä—à—Ä—É—Ç–æ–≤ —Å–æ–∑–¥–∞–Ω—ã:")
        print("      python generate_traffic.py --type all")
        print("   3. –ü–æ—Å–º–æ—Ç—Ä–∏ –≥—Ä–∞—Ñ–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è:")
        print("      ./logs/training_progress.png")


def check_training_progress():
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø—Ä–æ–≥—Ä–µ—Å—Å –∏–∑ –ª–æ–≥ —Ñ–∞–π–ª–∞"""
    log_file = f"{PROJECT_ROOT}/models/simple/logs/training_log.txt"

    if not os.path.exists(log_file):
        print("\n‚ö†Ô∏è –õ–æ–≥ —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω. –ú–æ–¥–µ–ª—å –µ—â–µ –Ω–µ –æ–±—É—á–∞–ª–∞—Å—å.")
        return

    print("\n" + "=" * 60)
    print("üìú –ò–°–¢–û–†–ò–Ø –û–ë–£–ß–ï–ù–ò–Ø")
    print("=" * 60)

    with open(log_file, "r") as f:
        lines = f.readlines()[1:]  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫

    if len(lines) < 10:
        print("‚ö†Ô∏è –°–ª–∏—à–∫–æ–º –º–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö. –ü—Ä–æ–¥–æ–ª–∂–∞–π –æ–±—É—á–µ–Ω–∏–µ.")
        return

    # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ –∏ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 10 —ç–ø–∏–∑–æ–¥–æ–≤
    early_episodes = lines[:10]
    late_episodes = lines[-10:]

    def parse_stats(episodes):
        rewards = [float(line.split(',')[2]) for line in episodes]
        bus_waits = [float(line.split(',')[3]) for line in episodes]
        return np.mean(rewards), np.mean(bus_waits)

    early_reward, early_bus = parse_stats(early_episodes)
    late_reward, late_bus = parse_stats(late_episodes)

    print(f"–ü–µ—Ä–≤—ã–µ 10 —ç–ø–∏–∑–æ–¥–æ–≤:")
    print(f"  –ù–∞–≥—Ä–∞–¥–∞: {early_reward:.1f}")
    print(f"  –û–∂–∏–¥–∞–Ω–∏–µ –∞–≤—Ç–æ–±—É—Å–æ–≤: {early_bus:.2f}s")

    print(f"\n–ü–æ—Å–ª–µ–¥–Ω–∏–µ 10 —ç–ø–∏–∑–æ–¥–æ–≤:")
    print(f"  –ù–∞–≥—Ä–∞–¥–∞: {late_reward:.1f}")
    print(f"  –û–∂–∏–¥–∞–Ω–∏–µ –∞–≤—Ç–æ–±—É—Å–æ–≤: {late_bus:.2f}s")

    reward_improvement = (late_reward - early_reward) / abs(early_reward) * 100
    bus_improvement = (early_bus - late_bus) / early_bus * 100

    print(f"\nüìä –ü—Ä–æ–≥—Ä–µ—Å—Å:")
    print(f"  –ù–∞–≥—Ä–∞–¥–∞: {reward_improvement:+.1f}%")
    print(f"  –û–∂–∏–¥–∞–Ω–∏–µ –∞–≤—Ç–æ–±—É—Å–æ–≤: {bus_improvement:+.1f}%")

    if reward_improvement > 20 and bus_improvement > 10:
        print("\n‚úÖ –ú–æ–¥–µ–ª—å —É—á–∏—Ç—Å—è! –ü—Ä–æ–¥–æ–ª–∂–∞–π –æ–±—É—á–µ–Ω–∏–µ.")
    elif reward_improvement > 0:
        print("\n‚ö†Ô∏è –ï—Å—Ç—å –ø—Ä–æ–≥—Ä–µ—Å—Å, –Ω–æ –º–µ–¥–ª–µ–Ω–Ω–æ. –í–æ–∑–º–æ–∂–Ω–æ –Ω—É–∂–Ω–æ –±–æ–ª—å—à–µ —à–∞–≥–æ–≤.")
    else:
        print("\n‚ùå –ù–µ—Ç –ø—Ä–æ–≥—Ä–µ—Å—Å–∞. –ü—Ä–æ–≤–µ—Ä—å –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–ª–∏ –Ω–∞—á–Ω–∏ –∑–∞–Ω–æ–≤–æ.")

    print("=" * 60)


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="–ë—ã—Å—Ç—Ä–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏")
    parser.add_argument("--check-progress", action="store_true",
                        help="–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –ø—Ä–æ–≥—Ä–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è –∏–∑ –ª–æ–≥–æ–≤")

    args = parser.parse_args()

    if args.check_progress:
        check_training_progress()
    else:
        compare_agents()